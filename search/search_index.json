{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This is a collection of the learnings I've had over the years. I've set up this repository using Obsidian and mkdocs to help me take my notebooks on the go.</p>"},{"location":"#general-programming","title":"General Programming","text":""},{"location":"#declarative-vs-imperative-programming-languages","title":"Declarative vs imperative programming languages","text":""},{"location":"#bitwise-operators","title":"Bitwise operators","text":""},{"location":"#bitwise-left-shift","title":"Bitwise Left shift","text":"<p>Bitwise left shift is the same as multiply by 2</p>"},{"location":"Kafka/","title":"Why Kafka?","text":"<p>As the applications grow and more nodes are added to each layer, the application soon turns into a mesh of connected services. In order to streamline our data flow, we introduce a Message Oriented Middleware(MOM)</p> <p>Kafka is a little more than a traditional MOM, it is a distributed event store and stream-processing platform written in Java and Scala. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems (for data import/export) via Kafka Connect, and provides the Kafka Streams libraries for stream processing applications. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a \"message set\" abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This \"leads to larger network packets, larger sequential disk operations, contiguous memory blocks which allows Kafka to turn a bursty stream of random message writes into linear writes.</p> <pre><code>flowchart TD\n\nA[Source System] --&gt;|Commit Data to| B(Database)\n</code></pre> <p>Soon this system turns into</p> <pre><code>flowchart TD\n\nA1[Source System] --&gt;B1(Database)\nA1[Source System] --&gt;B2(Analytics)\nA1[Source System] --&gt;B3(Audit)\nA2[Source System] --&gt;B1(Database)\nA2[Source System] --&gt;B2(Analytics)\nA2[Source System] --&gt;B3(Audit)\nA3[Source System] --&gt;B1(Database)\nA3[Source System] --&gt;B2(Analytics)\nA3[Source System] --&gt;B3(Audit)</code></pre> <p>In order to avoid mesh like this we introduce Kafka. It helps you decouple your data streams.</p> <pre><code>\nflowchart TD\nA1[Source]--&gt;K(Kafka)\nA2[Source]--&gt;K(Kafka)\nA3[Source]--&gt;K(Kafka)\nK--&gt;B1(Database)\nK--&gt;B2(Analytics)\nK--&gt;B3(Audit)</code></pre> <ul> <li>Distributed, resilient architecture, fault tolerant</li> <li>Horizontal Scalability</li> <li>High performance(&lt;10ms) ~realtime</li> <li>Can scale to &gt; millions of messages/second</li> </ul>"},{"location":"Kafka/#uses-of-kafka","title":"Uses of Kafka","text":"<ul> <li>Messaging System</li> <li>Activity tracking</li> <li>Gather metrics from various sources</li> <li>Application logs gatethering</li> <li>De-coupling system</li> <li>stream processing(Kafka Streams API or Spark)</li> <li>Integrations with Spark, flint, hadoop etc.</li> </ul>"},{"location":"Kafka/#kafka-ecosystem","title":"Kafka Ecosystem","text":"<pre><code>\nflowchart LR\n\nS[Source Systems]\nP[Producers]\nK[Kafka]\nC[Consumers]\nT[Target Systems]\nZ[Zookeeper]\nS --&gt; P\nP --&gt; K\nK --&gt; Z\nZ --&gt; K\nK --&gt; C\nC --&gt; T</code></pre>"},{"location":"Kafka/#kafka-extended-api","title":"Kafka Extended API","text":"<ul> <li>Kafka Connect + kafka streams + clster</li> </ul>"},{"location":"Kafka/#kafka-confluent-components","title":"Kafka confluent components","text":"<ul> <li>proprietary compoents from confluent.<ul> <li>Schema</li> <li>REST proxy</li> </ul> </li> </ul>"},{"location":"Kafka/#kafka-core-concepts-and-api","title":"Kafka core concepts and API","text":""},{"location":"Kafka/#kafka-in-the-enterprise-architecture","title":"Kafka in the enterprise architecture","text":"<p>![[Pasted image 20230501180846.png]]</p>"},{"location":"Kafka/#topics-and-partitions","title":"Topics and partitions","text":"<ul> <li>Topic is a stream of data.<ul> <li>Similar to table in a database(no constraints. More like a bucket in NoSQL)</li> <li>No limitations on topics.</li> <li>Topic is identified by their name</li> </ul> </li> <li> <p>Topic are split into partitions</p> <ul> <li>Each partition is ordered.</li> <li>Each message within partition gets an incremental ID called offset.  ![[Pasted image 20230501181201.png]]</li> </ul> </li> <li> <p>Offsets are relevant only for a particular partition.</p> </li> <li>Data is kept for a limited time(default is 2 weeks).</li> <li>Data written to partition is immutable.</li> <li>Data is pushed to topic. The partition is assigned randomly by hash manager.</li> <li>More partitions =&gt; more parallelizations.</li> </ul>"},{"location":"Kafka/#brokers","title":"Brokers","text":"<ul> <li>Kafka cluster is composed of multiple brokers(nodes/servers)</li> <li>Broker has ID</li> <li>Each broker contains certain topic partitions.</li> <li>You connect to 1 broker(called bootstrap broker). This enables us to connect with the entire cluster. ![[Pasted image 20230501181546.png]]</li> </ul>"},{"location":"Kafka/#topic-replication-factor","title":"Topic replication factor","text":"<ul> <li>Topic should have a replication factor &gt; 1(usually 2 or 3)</li> <li>In case 1 broker goes down ![[Pasted image 20230501181701.png]]</li> </ul>"},{"location":"Kafka/#partition-leader","title":"Partition leader","text":"<ul> <li>At any time, only 1 broker can act as a leader for a given partition.</li> <li>Only leader can receive and serve data for partition.</li> <li>Other replicas will sync with the data. ![[Pasted image 20230501181856.png]]</li> </ul>"},{"location":"Kafka/#producers","title":"Producers","text":"<ul> <li>Producer writes data to topics ![[Pasted image 20230501181932.png]]</li> </ul>"},{"location":"Kafka/#acks","title":"Acks","text":"Acks= Data loss probability 0 Don't wait for acknowledgement Possible 1 Producer will wait for leader acknowledgement Possible but Limited all Leader + Replica acknowledgement Most secure."},{"location":"Kafka/#message-keys","title":"Message Keys","text":"<ul> <li>Producer can choose to send a key</li> <li>Key ensures data goes to the same partition.</li> <li>This enables chronology in a distributed system.</li> </ul>"},{"location":"Kafka/#consumers","title":"Consumers","text":"<ul> <li>Read data from topic</li> <li>Consumes data in parallel across partitions. ![[Pasted image 20230501182517.png]]</li> </ul>"},{"location":"Kafka/#consumer-groups","title":"Consumer Groups","text":"<ul> <li>Each consumer within group reads from exclusive partitions</li> <li>No. of consumers !&gt; No. of partitions ![[Pasted image 20230501182657.png]]</li> </ul>"},{"location":"Kafka/#consumer-offsets","title":"Consumer offsets","text":"<ul> <li>Tells consumer where to read from</li> <li>Kafka stores the offset at which the consumer group is reading</li> <li>The offsets commit live in Kafka topic name \"__consumer_offsets\"</li> <li>When a consumer has processed data received on Kafka, it should commit the offsets.</li> <li>If a consumer process dies, it will be able to read back from last known location. </li> </ul>"},{"location":"Kafka/#zookeeper","title":"Zookeeper","text":""},{"location":"Redis/","title":"Scaling with Redis Cluster","text":"<p>https://redis.io/docs/reference/cluster-spec/ https://redis.io/docs/management/scaling/ https://redis.io/docs/reference/cluster-spec/#hash-tags</p> <ul> <li>Horizontal scaling with Redis is called a Redis cluster.</li> <li>High performance and linear scalability up to 1000 nodes. There are no proxies, asynchronous replication is used, and no merge operations are performed on values</li> <li>Redis cluster requires 2 ports to be open at every node<ul> <li>TCP Port: Default 6379</li> <li>Cluster Bus Port: Default 10000 + TCP port = 16379</li> </ul> </li> <li>To use Redis cluster on a single machine, we can use docker to map the ports such as all nodes internally run at the same ports while externally expose another set of ports</li> <li>Redis is, mostly, a single-threaded server from the POV of commands execution (actually modern versions of Redis use threads for different things). It is not designed to benefit from multiple CPU cores. People are supposed to launch several Redis instances to scale out on several cores if needed. It is not really fair to compare one single Redis instance to a multi-threaded data store</li> </ul>"},{"location":"Redis/#redis-cluster-data-sharding","title":"Redis cluster data sharding","text":"<p>Redis cluster doesn't use consistent hashing. Instead uses hash slot</p> <ul> <li>There are 16384 hash slots in Redis Cluster, and to compute the hash slot for a given key, we simply take the CRC16 of the key modulo 16384.<ul> <li>CRC = Cyclic redundancy check</li> </ul> </li> <li>Redis Cluster supports multiple key operations as long as all of the keys involved in a single command execution (or whole transaction, or Lua script execution) belong to the same hash slot. The user can force multiple keys to be part of the same hash slot by using a feature called hash tags.<ul> <li>the keys <code>user:{123}:profile</code> and <code>user:{123}:account</code> are guaranteed to be in the same hash slot because they share the same hash tag.</li> <li>More on hash tags: https://redis.io/docs/reference/cluster-spec/#hash-tags</li> </ul> </li> </ul>"},{"location":"Redis/#redis-cluster-master-replica-model","title":"Redis Cluster master-replica model","text":"<p>When the cluster is created (or at a later time), we add a replica node to every master, so that the final cluster is composed of A, B, C that are master nodes, and A1, B1, C1 that are replica nodes. This way, the system can continue if node B fails.</p> <p>Node B1 replicates B, and B fails, the cluster will promote node B1 as the new master and will continue to operate correctly.</p> <p>However, note that if nodes B and B1 fail at the same time, Redis Cluster will not be able to continue to operate.</p>"},{"location":"Redis/#redis-cluster-consistency-guarantees","title":"Redis Cluster consistency guarantees","text":"<ul> <li>Redis Cluster does not guarantee strong consistency. In practical terms this means that under certain conditions it is possible that Redis Cluster will lose writes that were acknowledged by the system to the client.</li> <li>The first reason why Redis Cluster can lose writes is because it uses asynchronous replication</li> <li>Basically, there is a trade-off to be made between performance and consistency.</li> <li>Redis Cluster has support for synchronous writes when absolutely needed, implemented via the <code>WAIT</code> command.<ul> <li>This makes losing writes a lot less likely. However, note that Redis Cluster does not implement strong consistency even when synchronous replication is used: it is always possible, under more complex failure scenarios, that a replica that was not able to receive the write will be elected as master.</li> </ul> </li> <li>This amount of time is a very important configuration directive of Redis Cluster, and is called the node timeout.</li> </ul>"},{"location":"Redis/#redis-cluster-configuration-parameters","title":"Redis Cluster configuration parameters","text":""},{"location":"CS50AI/Notebook/","title":"Notebook","text":""},{"location":"CS50AI/Notebook/#state","title":"State","text":"<p>A state in a problem represents the summary of the problem at current(or at time t) time.</p>"},{"location":"CS50AI/Notebook/#initial-state-s","title":"Initial State (\\(S\\))","text":"<p>Starting state of the problem. </p>"},{"location":"CS50AI/Notebook/#action-a","title":"Action (\\(A\\))","text":"<p>An action is defined as something that changes the state of the system from A to B.</p>"},{"location":"CS50AI/Notebook/#transition-model-t","title":"Transition Model (\\(T\\))","text":"<p>Transition model \\(T(S_n, A_i) =&gt;S_{n+1}\\) .</p> <p>Effectively, the transition model represents the action and the resultant state after the action is taken on the previous state.</p>"},{"location":"CS50AI/Notebook/#goal-test","title":"Goal Test","text":"<p>Goal test represents the final stage of the problem. In a simple case it can be 1 state. for more complicated cases, we can have various goals.</p>"},{"location":"CS50AI/Notebook/#path-cost-function","title":"Path cost function","text":"<p>Cost evaluation function to solve the problem. Can be a factor of money, time or any other resource we're trying to optimize.</p>"},{"location":"CS50AI/Notebook/#optimal-solution","title":"Optimal Solution","text":"<p>A solution that has the lowest path cost among all of the possible solutions.</p>"},{"location":"CS50AI/Notebook/#node","title":"Node","text":"<p>A node is a representation of the system at a particular point of time. - State:  - Parent - Action - Path cost</p>"},{"location":"CS50AI/Notebook/#search-strategies","title":"Search Strategies","text":""},{"location":"CS50AI/Notebook/#uninformed-search","title":"Uninformed search:","text":"<p>Blind BFS/DFS. We navigate just on the algorithm.</p>"},{"location":"CS50AI/Notebook/#informed-search","title":"Informed Search","text":"<p>There is some information from the system being shared with the search algo. This is then used to tweak the search algorithm.</p>"},{"location":"CS50AI/Notebook/#greedy-bfs","title":"Greedy BFS:","text":"<p>Only cares about the heurestic. <code>h(n)</code>. The heurestic is coded in picking up the next item from the frontier</p>"},{"location":"CS50AI/Notebook/#a-algorithm","title":"A* Algorithm","text":"<p>A* takes Greedy BFS to another level by adding the cost factor as well. <code>g(n) + h(n)</code></p>"},{"location":"CS50AI/Notebook/#adversarial-algorithms","title":"Adversarial Algorithms","text":"<p>When there are 2 parties involved working against each other.</p>"},{"location":"CS50AI/Notebook/#minimax-algorithm","title":"Minimax algorithm","text":""},{"location":"Docker/Basic/","title":"Basic","text":"<p>How to use docker</p>"},{"location":"Docker/buildx/","title":"Buildx","text":"<p>Docker buld kit is used to build the image for cross platform implementation.</p>"},{"location":"Docker/buildx/#how-to-setup-the-build-environment","title":"How to setup the build environment","text":"<pre><code>docker buildx create --use desktop-linux --config buildkitd.toml\n</code></pre> <p>Here <code>buildkitd.toml</code> is used to specify the configuration of the build environment. In my particular case, I have a self hosted docker image registry for my Kubernetes cluster. You can set up insecure registry url in the buildkit like this:</p> <pre><code>[registry.\"&lt;host&gt;:&lt;port&gt;\"]\nhttp = true\n</code></pre> <p>Reference</p>"},{"location":"General%20Linux/Networking/","title":"Networking","text":""},{"location":"General%20Linux/Networking/#how-to-access-system-on-intranet-as-hostnamelocal","title":"How to access system on intranet as <code>&lt;hostname&gt;.local</code>","text":"<p>This usually happens when the <code>libnss-mdns</code> is missing. Install it on Ubuntu using:</p> <pre><code>sudo apt install libnss-mdns\n</code></pre>"},{"location":"Kubernetes/Cheatsheet/","title":"Cheatsheet","text":""},{"location":"Kubernetes/Cheatsheet/#create-a-load-balancer","title":"Create a load balancer","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: argo-loadbalancer\n  namespace: default\nspec:\n  type: LoadBalancer\n  selector:\n    app.kubernetes.io/instance: my-argo-cd\n    app.kubernetes.io/name: argocd-server\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n</code></pre>"},{"location":"Kubernetes/Local%20Node%20Setup/","title":"Local Node Setup","text":"<p>I'll be using ubuntu to set up the local node in the cluster.</p> <p>The setup is generally going to be 2 masters and 1 node. Over a period of time, we can increase the masters.</p>"},{"location":"Kubernetes/Flux/Cheatsheet/","title":"Cheatsheet","text":""},{"location":"Kubernetes/Flux/Cheatsheet/#installing-flux","title":"Installing Flux","text":""},{"location":"Kubernetes/Flux/Cheatsheet/#how-to-monitor-the-kustomizations","title":"How to monitor the Kustomizations","text":"<pre><code>$ flux get kustomizations --watch\n</code></pre>"},{"location":"Languages/Rust/Basics/","title":"Why Rust?","text":""},{"location":"Languages/Rust/Vec/","title":"<code>vec.windows(usize)</code>","text":"<p>Runs the code while using <code>usize</code> items at a time. Good for comparision</p> <p>Eg:</p> <p>Check if the vec is sorted</p> <pre><code>vec![0, 1, 2, 563, 12123].windows(2).all(|a| a[0] &lt; a[1]);\n/// true\n\nvec![0, 1, 2, 1, 12].windows(2).all(|a| a[0] &lt; a[1]);\n/// false\n</code></pre>"},{"location":"Languages/Svelte/cheatsheet/","title":"Cheatsheet","text":""},{"location":"Languages/Svelte/cheatsheet/#start-a-new-project","title":"Start a new project","text":"<pre><code>npm create svelte@latest &lt;project_name&gt;\ncd &lt;project_name&gt;\nnpm install\nnpm run dev\n</code></pre>"},{"location":"Languages/Svelte/sveltekit/","title":"Sveltekit","text":""},{"location":"Languages/Svelte/sveltekit/#what-is-sveltekit","title":"What is <code>sveltekit</code>?","text":"<p><code>Sveltekit</code> is a build tool and framework for building web applications with Svelte, a popular JavaScript framework for building user interfaces. It provides a set of conventions and tools for building and deploying Svelte apps, making it easier for developers to get started with Svelte and to build and maintain large-scale applications. Sveltekit offers features such as automatic code splitting, server-side rendering, and optimized asset handling, all designed to help you build fast, scalable, and user-friendly web applications.</p>"},{"location":"Languages/Svelte/sveltekit/#project-structure","title":"Project Structure","text":"<pre><code>my-project/\n\u251c src/\n\u2502 \u251c lib/\n\u2502 \u2502 \u251c server/\n\u2502 \u2502 \u2502 \u2514 [your server-only lib files]\n\u2502 \u2502 \u2514 [your lib files]\n\u2502 \u251c params/\n\u2502 \u2502 \u2514 [your param matchers]\n\u2502 \u251c routes/\n\u2502 \u2502 \u2514 [your routes]\n\u2502 \u251c app.html\n\u2502 \u251c error.html\n\u2502 \u251c hooks.client.js\n\u2502 \u2514 hooks.server.js\n\u251c static/\n\u2502 \u2514 [your static assets]\n\u251c tests/\n\u2502 \u2514 [your tests]\n\u251c package.json\n\u251c svelte.config.js\n\u251c tsconfig.json\n\u2514 vite.config.js\n</code></pre>"},{"location":"Languages/mermaid/cheatsheet/","title":"mermaid.js","text":"<p><code>Mermaid.js</code> is an open source library which can be used to generate graphs in a textual format.</p>"},{"location":"Languages/mermaid/cheatsheet/#basics","title":"Basics","text":"<pre><code>flowchart LR\n    id1[(Database)]\n</code></pre> <pre><code>flowchart LR\n    id1[(Database)]</code></pre>"},{"location":"Languages/mkdocs/markdown/","title":"Adding a button","text":"<p>Adding a button requires you to add <code>{ .&lt;class_name&gt; }</code> to the link. It adds the button.</p> <pre><code>[Subscribe to our newsletter](#){ .md-button }\n</code></pre> <p>Generates the following button:</p> <p>Subscribe to our newsletter</p>"},{"location":"Languages/mkdocs/markdown/#adding-mermaid-graphs","title":"Adding mermaid graphs","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the  singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such  that \\(f(a)=f(b)\\).</p>"},{"location":"Languages/mkdocs/markdown/#embedding-code","title":"Embedding code","text":"<pre><code>```rs title=\"test.rs\" \nfn(){\n    dbg!(\"test\");\n}\n```\n</code></pre>"},{"location":"System%20Design/00-cheatsheet/","title":"Common layer across most applications","text":""},{"location":"System%20Design/00-cheatsheet/#authentication","title":"Authentication","text":""},{"location":"System%20Design/00-cheatsheet/#authorization","title":"Authorization","text":""},{"location":"System%20Design/00-cheatsheet/#transport-protocols","title":"Transport protocols","text":""},{"location":"System%20Design/00-cheatsheet/#httphttps","title":"HTTP/HTTPS","text":""},{"location":"System%20Design/00-cheatsheet/#hlsdash","title":"HLS/DASH:","text":"<p>HLS(HTTP Live Streaming) is an HTTP-based adaptive bitrate streaming protocol. Similar to MPEG-DASH or just DASH(Dynamic Adaptive Streaming over HTTP). However, DASH required </p>"},{"location":"System%20Design/00-cheatsheet/#domain-specific-knowledge","title":"Domain Specific knowledge","text":""},{"location":"System%20Design/00-cheatsheet/#streaminglive-streaming","title":"Streaming/Live Streaming","text":""},{"location":"System%20Design/00-cheatsheet/#segmentation","title":"Segmentation","text":""},{"location":"System%20Design/00-cheatsheet/#transcoding","title":"Transcoding","text":""},{"location":"System%20Design/00-cheatsheet/#practicals","title":"Practicals","text":""},{"location":"System%20Design/00-cheatsheet/#design-a-rate-limiter","title":"Design a rate limiter","text":"<p>Introduce as a middleware. Helps in keeping a state across deployment regions. In a possible scenario, we can also queue the pending messages. This can work in conjunction with some webhook for future communications.</p> <p>For distributed, use a centralized data store such as redis. Can perhaps use WATCH with redis transactions to acquire a lock to avoid race conditions.</p>"},{"location":"System%20Design/00-cheatsheet/#algorithms","title":"Algorithms:","text":""},{"location":"System%20Design/00-cheatsheet/#token-bucket","title":"Token Bucket","text":"<p>Easiest, recommended. Tokens are generated at some rate. Every request consumes one token. Extra tokens overflow</p>"},{"location":"System%20Design/00-cheatsheet/#leaky-bucket","title":"Leaky Bucket","text":"<p>Fixed length queue of requests. If queue full. Drop requests. Consume requests at a specific interval. Not advised for customer facing.</p>"},{"location":"System%20Design/00-cheatsheet/#fixed-window-counter","title":"Fixed Window counter","text":"<p>Window is fixed and requests are counted against it. Can lead to system abuse as the burst traffic is sometimes acceptable beyond limits.</p>"},{"location":"System%20Design/00-cheatsheet/#sliding-window-log","title":"Sliding Window log","text":"<p>Instead of a fixed window, we have a sliding window. Consumes lots of memory as it keeps a track of all the timestamps of the requests.</p>"},{"location":"System%20Design/00-cheatsheet/#sliding-window-counter","title":"Sliding Window Counter","text":"<p>Hybrid approach. </p>"},{"location":"System%20Design/01-basic-steps/","title":"01 basic steps","text":"<p>Right architecture is all about Picking the right battles and managing the trade-offs.</p>"},{"location":"System%20Design/01-basic-steps/#clarifying-the-scope-of-the-problem","title":"Clarifying the scope of the problem","text":"<p>System design is objective. A problem has multiple ways of approaching. However, one of the most important steps is to understand the requirements correctly.</p> <ul> <li>Use cases<ul> <li>Intended audience</li> <li>Audience scopes and distinction</li> </ul> </li> <li>Constraints<ul> <li>Identify the traffic and data handling constraints of the system in discussion</li> <li>Scale of the system such as requests per second, requests types, data written per second, data read per second)</li> <li>Special system requirements such as multi-threading, read or write oriented.</li> </ul> </li> </ul>"},{"location":"System%20Design/01-framework/","title":"01 framework","text":"<p>System design interviews are designed to be subjective. The problems are generally scaled massively and nobody really expects you to solve them. So what is it all about?</p> <p>Well, System design interviews are designed to evaluate the candidate's skill at designing a real-world software system with various components.</p> <p>The interviewer has worked and brewed the problem for weeks, if not months. What they are interested is your approach, your analytical skills and how do you approach a problem.</p> <p>Considering an interview usually lasts 60 minutes, introduction and closing questions leave us with just 40-50 minutes of time to work on the problem. So, it's crucial to have a framework in mind on how you approach the problem.</p>"},{"location":"System%20Design/01-framework/#understanding-the-problem-statement","title":"Understanding the problem statement","text":""},{"location":"System%20Design/01-framework/#high-level-design","title":"High level design","text":""},{"location":"System%20Design/01-framework/#deep-dive","title":"Deep Dive","text":""},{"location":"System%20Design/01-framework/#closing-statementsimprovementstradeoffs","title":"Closing statements/Improvements/Tradeoffs","text":"<ul> <li>Functional Requirements</li> <li>Non Functional Requirements</li> <li>Rough Estimations</li> <li>Communication interface/API</li> <li>High Level Design</li> <li>Data Modeling/ERD</li> <li>Authentication</li> <li>Authorization</li> <li>Security</li> <li>Reliability</li> <li>Scalibility</li> <li>Exception Handling</li> <li>Fault tolerance</li> <li>Error Handling</li> <li>Schema</li> <li>Monitoring</li> <li>Concurrency</li> <li>Latency</li> <li>Tradeoffs</li> </ul>"},{"location":"System%20Design/02-estimation/","title":"Availability Numbers","text":"Availability (%) Downtime/Day Downtime/Week Downtime/Month Downtime/Year 99% 14.40 min 1.68 hrs 7.31 hrs 3.65 days 99.99% 8.64 sec 1.01 min 4.38 min 52.60 min 99.999% 864.00 ms 6.05 sec 26.30 sec 5.26 min 99.9999% 86.40 ms 604.80 ms 2.63 sec 31.56 sec"},{"location":"System%20Design/02-estimation/#back-of-the-envelope-calculations","title":"Back of the envelope calculations","text":"Unit Approximation Bit 1 Byte 8 Kilobyte 10^3 B Megabyte 10^6 B Gigabyte 10^9 B Terabyte 10^12 B Petabyte 10^15 B <ul> <li>Seconds in a day = 60 * 60 * 24 = 86400</li> </ul> <p>1 Million events/day =&gt; 12 QPS i.e.</p> <p>for 20 million events/day = 12 * 20 = 240 QPS</p>"},{"location":"System%20Design/02-estimation/#framework","title":"Framework","text":"<ul> <li>Functional Requirements</li> <li>Non Functional Requirements</li> <li>Rough Estimations</li> <li>Communication interface/API</li> <li>High Level Design</li> <li>Data Modeling/ERD</li> <li>Authentication</li> <li>Authorization</li> <li>Security</li> <li>Reliability</li> <li>Scalibility</li> <li>Exception Handling</li> <li>Fault tolerance</li> <li>Error Handling</li> <li>Schema</li> <li>Monitoring</li> <li>Concurrency</li> <li>Latency</li> <li>Tradeoffs</li> </ul>"},{"location":"System%20Design/07-authentication/","title":"Authentication","text":"<p>We use various authentication models over the internet to protect the resources online.</p>"},{"location":"System%20Design/07-authentication/#rest-api-authentication","title":"REST API Authentication","text":"<p>To Read - https://stackoverflow.blog/2021/10/06/best-practices-for-authentication-and-authorization-for-rest-apis/ - https://mojoauth.com/blog/rest-api-authentication/ - https://www.openidentityplatform.org/blog/stateless-vs-stateful-authentication</p>"},{"location":"System%20Design/07-authentication/#basic-authentication","title":"Basic Authentication","text":"<p>The <code>username</code> and <code>password</code> are encoded in Base64(Generally). Widely used, however succeptible to attacks. Can be used over SSL and TLS channels. Doesn't allow MFA.</p>"},{"location":"System%20Design/07-authentication/#token-authentication","title":"Token Authentication","text":""},{"location":"System%20Design/07-authentication/#api-keys","title":"API Keys","text":"<p>Similar to Basic authentication. Prone to similar errors. Advantages being the API keys are decoupled from the user account. So the keys can be invalidated, expired and reissued as required.</p>"},{"location":"System%20Design/07-authentication/#hmac-encryption","title":"HMAC Encryption","text":"<p>Use HMAC(Hash based message authentication code) to encrypt the data. Use symmetric encryption. Client and server share key to ensure the integrity of payload. The payload can be intercepted the the malicious entity can attack</p>"},{"location":"System%20Design/07-authentication/#oauth-10","title":"OAuth 1.0","text":""},{"location":"System%20Design/07-authentication/#oauth-20","title":"OAuth 2.0","text":"<p>Considered Gold standard for REST APIs. Supports dynamic collections of users, permission levels, scope parameters and data types. OAuth 2.0 creates secured access tokens that are refreshed periodically using a series of procedural verification protocols known as grant types. The five major grant types in OAuth 2.0 are: -   Authorization Code -   Proof Key for Code Exchange (PKCE) -   Client Credentials -   Device Code -   Refresh Token</p> <p></p> <p>In addition to recycling access keys, OAuth supports the concept of scopes, a method of limiting an application's access to a user's account and associated credentials.</p>"},{"location":"System%20Design/07-authentication/#json-web-tokensjwt","title":"JSON Web Tokens(JWT)","text":"<p>Just like the Basic Header Authentication, JWT also pass a credential in the Authorization header. The difference is that the credential is the form of the token and that it can expire  - JWTs are stateless, all the information needed to authenticate a user is within that token. They can be used without a backing store. - Keep in mind that the token is sent every time a request is made. For enterprise apps, this could impact the data traffic size, as there would be multiple access levels, permissions and other roles to think about when implementing a solution with JWT. - To cut down on the request payload everytime, we can use opaque properties. i.e. instead of serialized data, we just use the references. i.e. <code>user_id: 123</code></p>"},{"location":"System%20Design/07-authentication/#single-sign-onsso","title":"Single Sign On(SSO)","text":"<ul> <li>Increases Security: No multiple accounts, no multiple points of failure or phishing as no login attempts.</li> <li>Increases Access: As users can login to multiple sites using just 1 identity</li> <li>Decreases admin overhead: As just 1 identity to manage.</li> </ul>"},{"location":"System%20Design/07-authentication/#openid-connect","title":"OpenID Connect","text":"<p>OpenID Connect is an open standard authentication protocol built on top of OAuth 2.0. It provides a simple way for a client application -- referred to as a relying party (RP) -- to validate a user's identity. OpenID connect does the same steps as Oauth2.0 until authorization code. However, in exchange of Authorization code, OIDC gets Access Token and ID Token. ID Token is usually JWT, carries Identification inforamtion about the user to the client.</p>"},{"location":"System%20Design/07-authentication/#oauth","title":"OAuth","text":"<p>OAuth 2.0 can also be used for SSO. We can create applications and use the identities from there. OAuth is used for Authorization. It needs to be used along with OIDC to be used as SSO</p>"},{"location":"System%20Design/07-authentication/#samlsecurity-assertion-markup-language","title":"SAML(Security Assertion Markup Language)","text":"<p>Based upon the Extensible Markup Language (XML) format, web applications use SAML to transfer authentication data between two parties - the identity provider (IdP) and the service provider (SP). <pre><code>\nflowchart LR\n\nA[User] --&gt; B(\"Identity Provider(iDP)\")\nA --&gt; C(\"Service Provider(SP)\")\nB --&gt; C</code></pre></p>"},{"location":"System%20Design/07-authentication/#soap-api-authentication","title":"SOAP API Authentication","text":""},{"location":"System%20Design/07-authentication/#server-to-server-api-authentication","title":"Server-to-Server API Authentication","text":""},{"location":"System%20Design/07-authentication/#client-device-long-term-session","title":"Client device long term session","text":""},{"location":"System%20Design/07-authentication/#_1","title":"Authentication","text":""}]}